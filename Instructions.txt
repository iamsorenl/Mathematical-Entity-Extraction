Here is a clean, copy-paste friendly breakdown of the assignment content from the PDF, suitable for placing in a `.txt` file:

---

**Assignment Summary: NLP 244 - Homework 2**
**Title: Mathematical Entity Extraction**

---

### 1. Objective

The goal is to build a system for **information extraction** from mathematical texts. The system should identify:

* **Entities**:

  * `definition`: defines a new concept or object.
  * `theorem`: a provable mathematical claim.
  * `proof`: the logical justification for a theorem.
  * `example`: an instance or application of a concept.

* **Identifiers (subtypes within entities only)**:

  * `name`: a label for a defined object or theorem (e.g., “Theorem 1.2”).
  * `reference`: a mention of a previously defined name.

⚠️ **Note**: Identifiers only appear inside other entities. Ignore entities inside proofs for better F1.

---

### 2. Instructions

#### 📌 **Part 1 – Required: Baseline Model (Few-Shot Prompting)**

Use prompting to assign **BIO tags** to tokens in the input text. Tags can be multi-label (e.g., a token can be both `definition` and `name`).

**Input**: Raw MMD-formatted mathematical text.
**Output**: A tag for each token (possibly multiple tags per token).

Example:

```
"least element" → [definition, name]
```

---

#### ⚙️ **Part 2 – Optional: Model Training**

Train a custom LLM on the provided dataset. Output should match the annotation format used in training data:

```
(start_char_index, end_char_index, annotation_type)
```

Model design can vary:

* Multi-label BIO tagging
* Combined class tags (e.g., theorem-definition)
* Ensemble tagging + merging outputs

📌 Base models suggested (but not required):

* LLaMA 3 (1B/3B/8B)
* Qwen 2/2.5
* Mistral 7B
* Any HuggingFace decoder-only model (but not smaller than BERT or too large)

---

#### 🚀 Inference Task

Use your trained model to label a set of **unannotated MMD files**.
**Final output** (in `.json` via `pandas.DataFrame`) must have:

```
Fields: fileid, start, end, tag
```

---

#### 🏆 Optional: Kaggle Leaderboard (Extra Credit)

Submit predictions to the class Kaggle board. Top 5 teams based on **token-level F1** receive extra credit:

| Rank | Bonus |
| ---- | ----- |
| 1st  | +5%   |
| 2nd  | +4%   |
| 3rd  | +3%   |
| 4th  | +2%   |
| 5th  | +1%   |

---

#### 🔍 Part 3 – Required: Error Analysis

Analyze errors in predictions, especially on unannotated MMD documents.

Questions to answer:

1. What mistakes does your model make? (give examples)
2. Why do these mistakes happen?
3. How could you improve the model? (go beyond “more data”)
4. What new challenges might your solution introduce?

---

### 3. Submission Checklist

Submit the following files (avoid files >50MB):

1. **Source code**, with clear instructions to:

   * Train your model
   * Run inference on a new MMD file

2. **Prediction files**:

   * On the test/validation set
   * On the unannotated MMD documents

3. **Report** (.pdf or .txt) including:

   * Model architecture, training details, data processing steps
   * Validation F1 scores for both baseline and trained models
   * Error analysis answering the Part 3 questions

---

Let me know if you want this turned into an actual `.txt` or `.md` file, or if you want me to scaffold out the source code/reports!


### Prompted chatGPT to summarize the assignment again and here is what they said:

Here is a **.txt-style summary of all instructions needed to complete the assignment**, formatted so you can easily copy it into a .txt file:

---

# Assignment 2 - Mathematical Entity Extraction (NLP 244)

## Objective

Build a system to extract:

* **Entities**: `definition`, `theorem`, `proof`, `example`
* **Identifiers**: `name`, `reference`

from undergraduate/graduate math textbooks (provided in MMD format).

## Input/Output

* **Input**: raw MMD text
* **Training Data**: (start index, end index, annotation type)
* **Output**:

  * BIO tags for each token (baseline method)
  * List of annotations for chunks of text (trained model)

## Part 1 - Required: Baseline Model (Few-shot prompting)

1. Use few-shot prompting with an LLM.
2. For each token, output one or more tags from:

   * `definition`, `theorem`, `proof`, `example`, `name`, `reference`
3. Example prompt template:

```
-- Instruction
Perform BIO tagging on the following snippet of text. Assign one or more labels for each token. You can choose from "definition", "theorem", "proof", "example", "name", or "reference".

-- Input
[your text here]

-- Output
[O] [O] ... [definition] [definition,name] ... [O] ...
```

4. Post-processing:

   * Names and references inside proofs should be filtered out.

## Part 2 - Optional: Training a Model

1. Fine-tune an LLM on provided training data.
2. Model input: chunk of MMD text (suggested: \~1024 tokens).
3. Model output: list of annotations (after post-processing), matching training data format.

### LLM Suggestions (but not required):

* LLaMA 3 (1B/3B/8B)
* Qwen 2/2.5 (1.5B/3B/7B)
* Mistral (7B)
* Any HuggingFace decoder-only model (larger than BERT, not >500B), ideally instruction-tuned and/or math-pretrained.

### Inference:

* Run model on provided unannotated documents.
* Save extracted annotations as a pandas DataFrame → JSON:

```json
Fields: fileid, start, end, tag
```

## Part 3 - Required: Analysis

For the baseline prompting method (and optionally for trained model), answer:

1. What kinds of mistakes does your model make? (cite examples)
2. Why does it make these mistakes?
3. How would you improve your model? (creative ideas)

   * Does your solution introduce new problems?

## Optional: Kaggle Leaderboard (Extra Credit)

* Submit predictions to Kaggle.
* Top 5 token-level F1 scores on combined test/validation get extra credit.

| Place | Bonus % |
| ----- | ------- |
| 1     | +5%     |
| 2     | +4%     |
| 3     | +3%     |
| 4     | +2%     |
| 5     | +1%     |

## Submission Checklist

Submit the following (NO big files >50 MB):

1. All source code + instructions for:

   * Running training (with best model/hyperparameters)
   * Running inference on MMD file + saving output
2. Your validation/test set predictions.
3. Your results on unannotated MMD files.
4. A report containing:

   * Detailed model description, training, data processing.
   * Token-level F1 scores on validation set for baseline & experimental models (highlight best score).
   * Error analysis answers (Part 3).

---

If you want, I can also generate:
✅ a **checklist version**
✅ a **starter project folder structure**
✅ a **prompt template you can copy/paste for baseline**
✅ an example **model training outline (code-friendly)**

Would you like me to? Just say which ones you want.
