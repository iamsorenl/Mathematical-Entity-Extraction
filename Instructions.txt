Here is a clean, copy-paste friendly breakdown of the assignment content from the PDF, suitable for placing in a `.txt` file:

---

**Assignment Summary: NLP 244 - Homework 2**
**Title: Mathematical Entity Extraction**

---

### 1. Objective

The goal is to build a system for **information extraction** from mathematical texts. The system should identify:

* **Entities**:

  * `definition`: defines a new concept or object.
  * `theorem`: a provable mathematical claim.
  * `proof`: the logical justification for a theorem.
  * `example`: an instance or application of a concept.

* **Identifiers (subtypes within entities only)**:

  * `name`: a label for a defined object or theorem (e.g., â€œTheorem 1.2â€).
  * `reference`: a mention of a previously defined name.

âš ï¸ **Note**: Identifiers only appear inside other entities. Ignore entities inside proofs for better F1.

---

### 2. Instructions

#### ðŸ“Œ **Part 1 â€“ Required: Baseline Model (Few-Shot Prompting)**

Use prompting to assign **BIO tags** to tokens in the input text. Tags can be multi-label (e.g., a token can be both `definition` and `name`).

**Input**: Raw MMD-formatted mathematical text.
**Output**: A tag for each token (possibly multiple tags per token).

Example:

```
"least element" â†’ [definition, name]
```

---

#### âš™ï¸ **Part 2 â€“ Optional: Model Training**

Train a custom LLM on the provided dataset. Output should match the annotation format used in training data:

```
(start_char_index, end_char_index, annotation_type)
```

Model design can vary:

* Multi-label BIO tagging
* Combined class tags (e.g., theorem-definition)
* Ensemble tagging + merging outputs

ðŸ“Œ Base models suggested (but not required):

* LLaMA 3 (1B/3B/8B)
* Qwen 2/2.5
* Mistral 7B
* Any HuggingFace decoder-only model (but not smaller than BERT or too large)

---

#### ðŸš€ Inference Task

Use your trained model to label a set of **unannotated MMD files**.
**Final output** (in `.json` via `pandas.DataFrame`) must have:

```
Fields: fileid, start, end, tag
```

---

#### ðŸ† Optional: Kaggle Leaderboard (Extra Credit)

Submit predictions to the class Kaggle board. Top 5 teams based on **token-level F1** receive extra credit:

| Rank | Bonus |
| ---- | ----- |
| 1st  | +5%   |
| 2nd  | +4%   |
| 3rd  | +3%   |
| 4th  | +2%   |
| 5th  | +1%   |

---

#### ðŸ” Part 3 â€“ Required: Error Analysis

Analyze errors in predictions, especially on unannotated MMD documents.

Questions to answer:

1. What mistakes does your model make? (give examples)
2. Why do these mistakes happen?
3. How could you improve the model? (go beyond â€œmore dataâ€)
4. What new challenges might your solution introduce?

---

### 3. Submission Checklist

Submit the following files (avoid files >50MB):

1. **Source code**, with clear instructions to:

   * Train your model
   * Run inference on a new MMD file

2. **Prediction files**:

   * On the test/validation set
   * On the unannotated MMD documents

3. **Report** (.pdf or .txt) including:

   * Model architecture, training details, data processing steps
   * Validation F1 scores for both baseline and trained models
   * Error analysis answering the Part 3 questions

---

Let me know if you want this turned into an actual `.txt` or `.md` file, or if you want me to scaffold out the source code/reports!


### Prompted chatGPT to summarize the assignment again and here is what they said:

Here is a **.txt-style summary of all instructions needed to complete the assignment**, formatted so you can easily copy it into a .txt file:

---

# Assignment 2 - Mathematical Entity Extraction (NLP 244)

## Objective

Build a system to extract:

* **Entities**: `definition`, `theorem`, `proof`, `example`
* **Identifiers**: `name`, `reference`

from undergraduate/graduate math textbooks (provided in MMD format).

## Input/Output

* **Input**: raw MMD text
* **Training Data**: (start index, end index, annotation type)
* **Output**:

  * BIO tags for each token (baseline method)
  * List of annotations for chunks of text (trained model)

## Part 1 - Required: Baseline Model (Few-shot prompting)

1. Use few-shot prompting with an LLM.
2. For each token, output one or more tags from:

   * `definition`, `theorem`, `proof`, `example`, `name`, `reference`
3. Example prompt template:

```
-- Instruction
Perform BIO tagging on the following snippet of text. Assign one or more labels for each token. You can choose from "definition", "theorem", "proof", "example", "name", or "reference".

-- Input
[your text here]

-- Output
[O] [O] ... [definition] [definition,name] ... [O] ...
```

4. Post-processing:

   * Names and references inside proofs should be filtered out.

## Part 2 - Optional: Training a Model

1. Fine-tune an LLM on provided training data.
2. Model input: chunk of MMD text (suggested: \~1024 tokens).
3. Model output: list of annotations (after post-processing), matching training data format.

### LLM Suggestions (but not required):

* LLaMA 3 (1B/3B/8B)
* Qwen 2/2.5 (1.5B/3B/7B)
* Mistral (7B)
* Any HuggingFace decoder-only model (larger than BERT, not >500B), ideally instruction-tuned and/or math-pretrained.

### Inference:

* Run model on provided unannotated documents.
* Save extracted annotations as a pandas DataFrame â†’ JSON:

```json
Fields: fileid, start, end, tag
```

## Part 3 - Required: Analysis

For the baseline prompting method (and optionally for trained model), answer:

1. What kinds of mistakes does your model make? (cite examples)
2. Why does it make these mistakes?
3. How would you improve your model? (creative ideas)

   * Does your solution introduce new problems?

## Optional: Kaggle Leaderboard (Extra Credit)

* Submit predictions to Kaggle.
* Top 5 token-level F1 scores on combined test/validation get extra credit.

| Place | Bonus % |
| ----- | ------- |
| 1     | +5%     |
| 2     | +4%     |
| 3     | +3%     |
| 4     | +2%     |
| 5     | +1%     |

## Submission Checklist

Submit the following (NO big files >50 MB):

1. All source code + instructions for:

   * Running training (with best model/hyperparameters)
   * Running inference on MMD file + saving output
2. Your validation/test set predictions.
3. Your results on unannotated MMD files.
4. A report containing:

   * Detailed model description, training, data processing.
   * Token-level F1 scores on validation set for baseline & experimental models (highlight best score).
   * Error analysis answers (Part 3).

---

If you want, I can also generate:
âœ… a **checklist version**
âœ… a **starter project folder structure**
âœ… a **prompt template you can copy/paste for baseline**
âœ… an example **model training outline (code-friendly)**

Would you like me to? Just say which ones you want.
