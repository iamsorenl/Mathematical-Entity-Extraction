Here is a clean, copy-paste friendly breakdown of the assignment content from the PDF, suitable for placing in a `.txt` file:

---

**Assignment Summary: NLP 244 - Homework 2**
**Title: Mathematical Entity Extraction**

---

### 1. Objective

The goal is to build a system for **information extraction** from mathematical texts. The system should identify:

* **Entities**:

  * `definition`: defines a new concept or object.
  * `theorem`: a provable mathematical claim.
  * `proof`: the logical justification for a theorem.
  * `example`: an instance or application of a concept.

* **Identifiers (subtypes within entities only)**:

  * `name`: a label for a defined object or theorem (e.g., â€œTheorem 1.2â€).
  * `reference`: a mention of a previously defined name.

âš ï¸ **Note**: Identifiers only appear inside other entities. Ignore entities inside proofs for better F1.

---

### 2. Instructions

#### ðŸ“Œ **Part 1 â€“ Required: Baseline Model (Few-Shot Prompting)**

Use prompting to assign **BIO tags** to tokens in the input text. Tags can be multi-label (e.g., a token can be both `definition` and `name`).

**Input**: Raw MMD-formatted mathematical text.
**Output**: A tag for each token (possibly multiple tags per token).

Example:

```
"least element" â†’ [definition, name]
```

---

#### âš™ï¸ **Part 2 â€“ Optional: Model Training**

Train a custom LLM on the provided dataset. Output should match the annotation format used in training data:

```
(start_char_index, end_char_index, annotation_type)
```

Model design can vary:

* Multi-label BIO tagging
* Combined class tags (e.g., theorem-definition)
* Ensemble tagging + merging outputs

ðŸ“Œ Base models suggested (but not required):

* LLaMA 3 (1B/3B/8B)
* Qwen 2/2.5
* Mistral 7B
* Any HuggingFace decoder-only model (but not smaller than BERT or too large)

---

#### ðŸš€ Inference Task

Use your trained model to label a set of **unannotated MMD files**.
**Final output** (in `.json` via `pandas.DataFrame`) must have:

```
Fields: fileid, start, end, tag
```

---

#### ðŸ† Optional: Kaggle Leaderboard (Extra Credit)

Submit predictions to the class Kaggle board. Top 5 teams based on **token-level F1** receive extra credit:

| Rank | Bonus |
| ---- | ----- |
| 1st  | +5%   |
| 2nd  | +4%   |
| 3rd  | +3%   |
| 4th  | +2%   |
| 5th  | +1%   |

---

#### ðŸ” Part 3 â€“ Required: Error Analysis

Analyze errors in predictions, especially on unannotated MMD documents.

Questions to answer:

1. What mistakes does your model make? (give examples)
2. Why do these mistakes happen?
3. How could you improve the model? (go beyond â€œmore dataâ€)
4. What new challenges might your solution introduce?

---

### 3. Submission Checklist

Submit the following files (avoid files >50MB):

1. **Source code**, with clear instructions to:

   * Train your model
   * Run inference on a new MMD file

2. **Prediction files**:

   * On the test/validation set
   * On the unannotated MMD documents

3. **Report** (.pdf or .txt) including:

   * Model architecture, training details, data processing steps
   * Validation F1 scores for both baseline and trained models
   * Error analysis answering the Part 3 questions

---

Let me know if you want this turned into an actual `.txt` or `.md` file, or if you want me to scaffold out the source code/reports!
