Here is a clean, copy-paste friendly breakdown of the assignment content from the PDF, suitable for placing in a `.txt` file:

---

**Assignment Summary: NLP 244 - Homework 2**
**Title: Mathematical Entity Extraction**

---

### 1. Objective

The goal is to build a system for **information extraction** from mathematical texts. The system should identify:

* **Entities**:

  * `definition`: defines a new concept or object.
  * `theorem`: a provable mathematical claim.
  * `proof`: the logical justification for a theorem.
  * `example`: an instance or application of a concept.

* **Identifiers (subtypes within entities only)**:

  * `name`: a label for a defined object or theorem (e.g., “Theorem 1.2”).
  * `reference`: a mention of a previously defined name.

⚠️ **Note**: Identifiers only appear inside other entities. Ignore entities inside proofs for better F1.

---

### 2. Instructions

#### 📌 **Part 1 – Required: Baseline Model (Few-Shot Prompting)**

Use prompting to assign **BIO tags** to tokens in the input text. Tags can be multi-label (e.g., a token can be both `definition` and `name`).

**Input**: Raw MMD-formatted mathematical text.
**Output**: A tag for each token (possibly multiple tags per token).

Example:

```
"least element" → [definition, name]
```

---

#### ⚙️ **Part 2 – Optional: Model Training**

Train a custom LLM on the provided dataset. Output should match the annotation format used in training data:

```
(start_char_index, end_char_index, annotation_type)
```

Model design can vary:

* Multi-label BIO tagging
* Combined class tags (e.g., theorem-definition)
* Ensemble tagging + merging outputs

📌 Base models suggested (but not required):

* LLaMA 3 (1B/3B/8B)
* Qwen 2/2.5
* Mistral 7B
* Any HuggingFace decoder-only model (but not smaller than BERT or too large)

---

#### 🚀 Inference Task

Use your trained model to label a set of **unannotated MMD files**.
**Final output** (in `.json` via `pandas.DataFrame`) must have:

```
Fields: fileid, start, end, tag
```

---

#### 🏆 Optional: Kaggle Leaderboard (Extra Credit)

Submit predictions to the class Kaggle board. Top 5 teams based on **token-level F1** receive extra credit:

| Rank | Bonus |
| ---- | ----- |
| 1st  | +5%   |
| 2nd  | +4%   |
| 3rd  | +3%   |
| 4th  | +2%   |
| 5th  | +1%   |

---

#### 🔍 Part 3 – Required: Error Analysis

Analyze errors in predictions, especially on unannotated MMD documents.

Questions to answer:

1. What mistakes does your model make? (give examples)
2. Why do these mistakes happen?
3. How could you improve the model? (go beyond “more data”)
4. What new challenges might your solution introduce?

---

### 3. Submission Checklist

Submit the following files (avoid files >50MB):

1. **Source code**, with clear instructions to:

   * Train your model
   * Run inference on a new MMD file

2. **Prediction files**:

   * On the test/validation set
   * On the unannotated MMD documents

3. **Report** (.pdf or .txt) including:

   * Model architecture, training details, data processing steps
   * Validation F1 scores for both baseline and trained models
   * Error analysis answering the Part 3 questions

---

Let me know if you want this turned into an actual `.txt` or `.md` file, or if you want me to scaffold out the source code/reports!
